{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3fa940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load D:\\python_script\\soft-filter-pruning-master\\pruning_cifar10_resnet.py\n",
    "from __future__ import division\n",
    "\n",
    "import os, sys, shutil, time, random\n",
    "import argparse\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from utils import AverageMeter, RecorderMeter, time_string, convert_secs2time\n",
    "import models\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "model_names = sorted(name for name in models.__dict__\n",
    "    if name.islower() and not name.startswith(\"__\")\n",
    "    and callable(models.__dict__[name]))\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Trains ResNeXt on CIFAR or ImageNet', formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "parser.add_argument('data_path', type=str, help='Path to dataset')\n",
    "parser.add_argument('--dataset', type=str, choices=['cifar10', 'cifar100', 'imagenet', 'svhn', 'stl10'], help='Choose between Cifar10/100 and ImageNet.')\n",
    "parser.add_argument('--arch', metavar='ARCH', default='resnet18', choices=model_names, help='model architecture: ' + ' | '.join(model_names) + ' (default: resnext29_8_64)')\n",
    "# Optimization options\n",
    "parser.add_argument('--epochs', type=int, default=300, help='Number of epochs to train.')\n",
    "parser.add_argument('--batch_size', type=int, default=128, help='Batch size.')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.1, help='The Learning Rate.')\n",
    "parser.add_argument('--momentum', type=float, default=0.9, help='Momentum.')\n",
    "parser.add_argument('--decay', type=float, default=0.0005, help='Weight decay (L2 penalty).')\n",
    "parser.add_argument('--schedule', type=int, nargs='+', default=[150, 225], help='Decrease learning rate at these epochs.')\n",
    "parser.add_argument('--gammas', type=float, nargs='+', default=[0.1, 0.1], help='LR is multiplied by gamma on schedule, number of gammas should be equal to schedule')\n",
    "# Checkpoints\n",
    "parser.add_argument('--print_freq', default=200, type=int, metavar='N', help='print frequency (default: 200)')\n",
    "parser.add_argument('--save_path', type=str, default='./', help='Folder to save checkpoints and log.')\n",
    "parser.add_argument('--resume', default='', type=str, metavar='PATH', help='path to latest checkpoint (default: none)')\n",
    "parser.add_argument('--start_epoch', default=0, type=int, metavar='N', help='manual epoch number (useful on restarts)')\n",
    "parser.add_argument('--evaluate', dest='evaluate', action='store_true', help='evaluate model on validation set')\n",
    "# Acceleration\n",
    "parser.add_argument('--ngpu', type=int, default=1, help='0 = CPU.')\n",
    "parser.add_argument('--workers', type=int, default=2, help='number of data loading workers (default: 2)')\n",
    "# random seed\n",
    "parser.add_argument('--manualSeed', type=int, help='manual seed')\n",
    "# compress rate\n",
    "parser.add_argument('--rate', type=float, default=0.9, help='compress rate of model')\n",
    "parser.add_argument('--layer_begin', type=int, default=1,  help='compress layer of model')\n",
    "parser.add_argument('--layer_end', type=int, default=1,  help='compress layer of model')\n",
    "parser.add_argument('--layer_inter', type=int, default=1,  help='compress layer of model')\n",
    "parser.add_argument('--epoch_prune', type=int, default=1,  help='compress layer of model')\n",
    "parser.add_argument('--use_state_dict', dest='use_state_dict', action='store_true', help='use state dict or not')\n",
    "\n",
    "\n",
    "args = parser.parse_args()\n",
    "args.use_cuda = args.ngpu>0 and torch.cuda.is_available()\n",
    "\n",
    "# 设置随机种子\n",
    "if args.manualSeed is None:\n",
    "    args.manualSeed = random.randint(1, 10000)\n",
    "random.seed(args.manualSeed)\n",
    "torch.manual_seed(args.manualSeed)\n",
    "if args.use_cuda:\n",
    "    torch.cuda.manual_seed_all(args.manualSeed)\n",
    "cudnn.benchmark = True\n",
    "\n",
    "def main():\n",
    "    # Init logger  初始化日志\n",
    "    if not os.path.isdir(args.save_path):\n",
    "        os.makedirs(args.save_path)\n",
    "    log = open(os.path.join(args.save_path, 'log_seed_{}.txt'.format(args.manualSeed)), 'w')\n",
    "    print_log('save path : {}'.format(args.save_path), log)\n",
    "    state = {k: v for k, v in args._get_kwargs()}  # 获取args中的关键字参数（关键字参数在函数中是字典类型的）\n",
    "    print_log(state, log)\n",
    "    print_log(\"Random Seed: {}\".format(args.manualSeed), log)\n",
    "    print_log(\"python version : {}\".format(sys.version.replace('\\n', ' ')), log)\n",
    "    print_log(\"torch  version : {}\".format(torch.__version__), log)\n",
    "    print_log(\"cudnn  version : {}\".format(torch.backends.cudnn.version()), log)\n",
    "    print_log(\"Compress Rate: {}\".format(args.rate), log)\n",
    "    print_log(\"Layer Begin: {}\".format(args.layer_begin), log)\n",
    "    print_log(\"Layer End: {}\".format(args.layer_end), log)\n",
    "    print_log(\"Layer Inter: {}\".format(args.layer_inter), log)\n",
    "    print_log(\"Epoch prune: {}\".format(args.epoch_prune), log)\n",
    "    # Init dataset 初始化数据集\n",
    "    if not os.path.isdir(args.data_path):\n",
    "        os.makedirs(args.data_path)\n",
    "\n",
    "    if args.dataset == 'cifar10':\n",
    "        mean = [x / 255 for x in [125.3, 123.0, 113.9]]\n",
    "        std = [x / 255 for x in [63.0, 62.1, 66.7]]\n",
    "    elif args.dataset == 'cifar100':\n",
    "        mean = [x / 255 for x in [129.3, 124.1, 112.4]]\n",
    "        std = [x / 255 for x in [68.2, 65.4, 70.4]]\n",
    "    else:\n",
    "        assert False, \"Unknow dataset : {}\".format(args.dataset)  # assert后为真，则继续向下进行，否则输出后面的报错信息\n",
    "# 训练（测试）的数据转换操作，进行串联\n",
    "    train_transform = transforms.Compose(  # 将多个转换器进行串联\n",
    "        [transforms.RandomHorizontalFlip(),  # 随机水平翻转给定的图片，概率为0.5\n",
    "         transforms.RandomCrop(32, padding=4),  # 随机选择裁剪的中心点，padding增加的宽度\n",
    "         transforms.ToTensor(),  # 数据格式转换为tensor\n",
    "         transforms.Normalize(mean, std)])  # 通过给定的均值方差，将tensor进行正则化\n",
    "    test_transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),  # 数据格式转换成tensor\n",
    "         transforms.Normalize(mean, std)])  # 同正则化\n",
    "\n",
    "    if args.dataset == 'cifar10':\n",
    "        # root=cifar-10-batches-py的根目录，train=是训练集，transform=数据的转换操作，download=从网络下载数据\n",
    "        train_data = dset.CIFAR10(args.data_path, train=True, transform=train_transform, download=False)\n",
    "        test_data = dset.CIFAR10(args.data_path, train=False, transform=test_transform, download=False)\n",
    "        num_classes = 10\n",
    "    elif args.dataset == 'cifar100':\n",
    "        train_data = dset.CIFAR100(args.data_path, train=True, transform=train_transform, download=True)\n",
    "        test_data = dset.CIFAR100(args.data_path, train=False, transform=test_transform, download=True)\n",
    "        num_classes = 100\n",
    "    elif args.dataset == 'svhn':\n",
    "        train_data = dset.SVHN(args.data_path, split='train', transform=train_transform, download=True)\n",
    "        test_data = dset.SVHN(args.data_path, split='test', transform=test_transform, download=True)\n",
    "        num_classes = 10\n",
    "    elif args.dataset == 'stl10':\n",
    "        train_data = dset.STL10(args.data_path, split='train', transform=train_transform, download=True)\n",
    "        test_data = dset.STL10(args.data_path, split='test', transform=test_transform, download=True)\n",
    "        num_classes = 10\n",
    "    elif args.dataset == 'imagenet':\n",
    "        assert False, 'Do not finish imagenet code'  # assert后为真，则继续向下进行，否则输出后面的报错信息\n",
    "    else:\n",
    "        assert False, 'Do not support dataset : {}'.format(args.dataset)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=args.batch_size, shuffle=True,\n",
    "                                                 num_workers=args.workers, pin_memory=True)  # 数据加载器\n",
    "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=args.batch_size, shuffle=False,\n",
    "                                                num_workers=args.workers, pin_memory=True)\n",
    "\n",
    "    print_log(\"=> creating model '{}'\".format(args.arch), log)\n",
    "    # Init model, criterion, and optimizer\n",
    "    net = models.__dict__[args.arch](num_classes)  # 设置（加载）网络模型\n",
    "    print_log(\"=> network :\\n {}\".format(net), log)\n",
    "\n",
    "    net = torch.nn.DataParallel(net, device_ids=list(range(args.ngpu)))  # GPU并行计算\n",
    "\n",
    "\n",
    "\n",
    "    # define loss function (criterion标准) and optimizer\n",
    "    criterion = torch.nn.CrossEntropyLoss()  # 损失函数为交叉熵损失函数\n",
    "\n",
    "    optimizer = torch.optim.SGD(net.parameters(), state['learning_rate'], momentum=state['momentum'],\n",
    "                                weight_decay=state['decay'], nesterov=True)  # 优化器\n",
    "\n",
    "    if args.use_cuda:  #将数据放置在GPU中\n",
    "        net.cuda()\n",
    "        criterion.cuda()\n",
    "\n",
    "    recorder = RecorderMeter(args.epochs)  # 计算并记录最小损失函数和它的epoch\n",
    "    # optionally resume from a checkpoint  从断点处继续训练模型（选择性的）\n",
    "    if args.resume:\n",
    "        if os.path.isfile(args.resume):\n",
    "            print_log(\"=> loading checkpoint '{}'\".format(args.resume), log)\n",
    "            checkpoint = torch.load(args.resume)  # 从磁盘中读取文件\n",
    "            recorder = checkpoint['recorder']\n",
    "            args.start_epoch = checkpoint['epoch']  # 将新的断点处作为新的epoch进行训练\n",
    "            if args.use_state_dict:\n",
    "                net.load_state_dict(checkpoint['state_dict'])  # 将state_dict中的parameters和buffers复制到它的后代中(重载)\n",
    "            else:\n",
    "                net = checkpoint['state_dict']\n",
    "                \n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])  # 加载optimizer的状态（重载）\n",
    "            print_log(\"=> loaded checkpoint '{}' (epoch {})\" .format(args.resume, checkpoint['epoch']), log)\n",
    "        else:\n",
    "            print_log(\"=> no checkpoint found at '{}'\".format(args.resume), log)\n",
    "    else:\n",
    "        print_log(\"=> do not use any checkpoint for {} model\".format(args.arch), log)\n",
    "\n",
    "    if args.evaluate:  # 在验证集上进行计算\n",
    "        time1 = time.time()\n",
    "        validate(test_loader, net, criterion, log)\n",
    "        time2 = time.time()\n",
    "        print ('function took %0.3f ms' % ((time2-time1)*1000.0))\n",
    "        return\n",
    "\n",
    "    m=Mask(net)\n",
    "    \n",
    "    m.init_length()\n",
    "    \n",
    "    comp_rate = args.rate\n",
    "    print(\"-\"*10+\"one epoch begin\"+\"-\"*10)\n",
    "    print(\"the compression rate now is %f\" % comp_rate)\n",
    "\n",
    "    val_acc_1,   val_los_1  = validate(test_loader, net, criterion, log)\n",
    "\n",
    "    print(\" accu before is: %.3f %%\" % val_acc_1)\n",
    "    \n",
    "    m.model = net\n",
    "    \n",
    "    m.init_mask(comp_rate)\n",
    "#    m.if_zero()\n",
    "    m.do_mask()\n",
    "    net = m.model\n",
    "#    m.if_zero()\n",
    "    if args.use_cuda:\n",
    "        net = net.cuda()\n",
    "    val_acc_2, val_los_2 = validate(test_loader, net, criterion, log)\n",
    "    print(\" accu after is: %s %%\" % val_acc_2)\n",
    "    \n",
    "\n",
    "    # Main loop\n",
    "    start_time = time.time()\n",
    "    epoch_time = AverageMeter()\n",
    "    for epoch in range(args.start_epoch, args.epochs):\n",
    "        current_learning_rate = adjust_learning_rate(optimizer, epoch, args.gammas, args.schedule)\n",
    "\n",
    "        need_hour, need_mins, need_secs = convert_secs2time(epoch_time.avg * (args.epochs-epoch))\n",
    "        need_time = '[Need: {:02d}:{:02d}:{:02d}]'.format(need_hour, need_mins, need_secs)\n",
    "\n",
    "        print_log('\\n==>>{:s} [Epoch={:03d}/{:03d}] {:s} [learning_rate={:6.4f}]'.format(time_string(), epoch, args.epochs, need_time, current_learning_rate) \\\n",
    "                                + ' [Best : Accuracy={:.2f}, Error={:.2f}]'.format(recorder.max_accuracy(False), 100-recorder.max_accuracy(False)), log)\n",
    "\n",
    "        # train for one epoch\n",
    "        train_acc, train_los = train(train_loader, net, criterion, optimizer, epoch, log)\n",
    "\n",
    "        # evaluate on validation set\n",
    "        val_acc_1, val_los_1 = validate(test_loader, net, criterion, log)\n",
    "        # 是否进行剪枝\n",
    "        if (epoch % args.epoch_prune ==0 or epoch == args.epochs-1):\n",
    "            m.model = net\n",
    "            m.if_zero()\n",
    "            m.init_mask(comp_rate)\n",
    "            m.do_mask()\n",
    "            m.if_zero()\n",
    "            net = m.model\n",
    "            if args.use_cuda:\n",
    "                net = net.cuda()  \n",
    "            \n",
    "        val_acc_2, val_los_2 = validate(test_loader, net, criterion, log)\n",
    "    \n",
    "        \n",
    "        is_best = recorder.update(epoch, train_los, train_acc, val_los_2, val_acc_2)\n",
    "        # 保存检查点\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'arch': args.arch,\n",
    "            'state_dict': net,\n",
    "            'recorder': recorder,\n",
    "            'optimizer' : optimizer.state_dict(),\n",
    "        }, is_best, args.save_path, 'checkpoint.pth.tar')\n",
    "\n",
    "        # measure elapsed time\n",
    "        epoch_time.update(time.time() - start_time)\n",
    "        start_time = time.time()\n",
    "        #recorder.plot_curve( os.path.join(args.save_path, 'curve.png') )\n",
    "\n",
    "    log.close()\n",
    "\n",
    "# train function (forward向前传播, backward向后传播, update更新权重)\n",
    "def train(train_loader, model, criterion, optimizer, epoch, log):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        if args.use_cuda:\n",
    "            target = target.cuda(non_blocking = True)\n",
    "            input = input.cuda()\n",
    "        input_var = torch.autograd.Variable(input)  # 包装一个tensor，同时记录在它身上的操作\n",
    "        target_var = torch.autograd.Variable(target)\n",
    "\n",
    "        # compute output\n",
    "        output = model(input_var)\n",
    "        loss = criterion(output, target_var)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n",
    "        losses.update(loss.data[0], input.size(0))\n",
    "        top1.update(prec1[0], input.size(0))\n",
    "        top5.update(prec5[0], input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed（流逝） time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % args.print_freq == 0:\n",
    "            print_log('  Epoch: [{:03d}][{:03d}/{:03d}]   '\n",
    "                        'Time {batch_time.val:.3f} ({batch_time.avg:.3f})   '\n",
    "                        'Data {data_time.val:.3f} ({data_time.avg:.3f})   '\n",
    "                        'Loss {loss.val:.4f} ({loss.avg:.4f})   '\n",
    "                        'Prec@1 {top1.val:.3f} ({top1.avg:.3f})   '\n",
    "                        'Prec@5 {top5.val:.3f} ({top5.avg:.3f})   '.format(\n",
    "                        epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                        data_time=data_time, loss=losses, top1=top1, top5=top5) + time_string(), log)\n",
    "    print_log('  **Train** Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f} Error@1 {error1:.3f}'.format(top1=top1, top5=top5, error1=100-top1.avg), log)\n",
    "    return top1.avg, losses.avg\n",
    "\n",
    "def validate(val_loader, model, criterion, log):\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    for i, (input, target) in enumerate(val_loader):\n",
    "        if args.use_cuda:\n",
    "            target = target.cuda(non_blocking = True)\n",
    "            input = input.cuda()\n",
    "        input_var = torch.autograd.Variable(input, volatile=True)\n",
    "        target_var = torch.autograd.Variable(target, volatile=True)\n",
    "\n",
    "        # compute output\n",
    "        output = model(input_var)\n",
    "        loss = criterion(output, target_var)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n",
    "        losses.update(loss.data[0], input.size(0))\n",
    "        top1.update(prec1[0], input.size(0))\n",
    "        top5.update(prec5[0], input.size(0))\n",
    "\n",
    "    print_log('  **Test** Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f} Error@1 {error1:.3f}'.format(top1=top1, top5=top5, error1=100-top1.avg), log)\n",
    "\n",
    "    return top1.avg, losses.avg\n",
    "\n",
    "def print_log(print_string, log):\n",
    "    print(\"{}\".format(print_string))\n",
    "    log.write('{}\\n'.format(print_string))\n",
    "    log.flush()\n",
    "\n",
    "def save_checkpoint(state, is_best, save_path, filename):\n",
    "    filename = os.path.join(save_path, filename)\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        bestname = os.path.join(save_path, 'model_best.pth.tar')\n",
    "        shutil.copyfile(filename, bestname)\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch, gammas, schedule):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    lr = args.learning_rate\n",
    "    assert len(gammas) == len(schedule), \"length of gammas and schedule should be equal\"\n",
    "    for (gamma, step) in zip(gammas, schedule):\n",
    "        if (epoch >= step):\n",
    "            lr = lr * gamma\n",
    "        else:\n",
    "            break\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return lr\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "class Mask:\n",
    "    def __init__(self, model):\n",
    "        self.model_size = {}\n",
    "        self.model_length = {}\n",
    "        self.compress_rate = {}\n",
    "        self.mat = {}\n",
    "        self.model = model\n",
    "        self.mask_index = []\n",
    "        \n",
    "    \n",
    "    def get_codebook(self, weight_torch,compress_rate,length):\n",
    "        weight_vec = weight_torch.view(length)\n",
    "        weight_np = weight_vec.cpu().numpy()\n",
    "    \n",
    "        weight_abs = np.abs(weight_np)  # 取权重的绝对值\n",
    "        weight_sort = np.sort(weight_abs)  # 对权重进行排序\n",
    "        \n",
    "        threshold = weight_sort[int (length * (1-compress_rate))]  # 剪枝的权重临界值\n",
    "        # 进行权重剪枝\n",
    "        weight_np[weight_np <= -threshold] = 1\n",
    "        weight_np[weight_np >= threshold] = 1\n",
    "        weight_np[weight_np !=1] = 0\n",
    "        \n",
    "        print(\"codebook done\")\n",
    "        return weight_np    # 返回剪枝二进制向量\n",
    "\n",
    "    def get_filter_codebook(self, weight_torch,compress_rate,length):\n",
    "        codebook = np.ones(length)\n",
    "        if len(weight_torch.size()) == 4:  # 权重值为四维向量（a,b,c,d），a为层数，b为某一层卷积核数量，b=c为卷积核大小，（待定）\n",
    "            filter_pruned_num = int(weight_torch.size()[0]*(1-compress_rate))\n",
    "            weight_vec = weight_torch.view(weight_torch.size()[0],-1)  # 维度变换为（a,bcd）\n",
    "            norm2 = torch.norm(weight_vec,2,1)  # 此处P=2,表示计算的是2范数，dim=1表示进行压缩的是第二维度，即对每个卷积核进行计算其结果的维度是（a,c,d）\n",
    "            norm2_np = norm2.cpu().numpy()  # 转换成numpy格式\n",
    "            filter_index = norm2_np.argsort()[:filter_pruned_num]  # argsort返回数组值从小到达的索引值,每个维度的排序是分开的,取应剪枝的数量\n",
    "#            norm1_sort = np.sort(norm1_np)\n",
    "#            threshold = norm1_sort[int (weight_torch.size()[0] * (1-compress_rate) )]\n",
    "            kernel_length = weight_torch.size()[1] *weight_torch.size()[2] *weight_torch.size()[3]  # 计算核的长度，即卷积核的参数b*c*d（待定）\n",
    "            # 进行剪枝操作\n",
    "            for x in range(0,len(filter_index)):\n",
    "                codebook[filter_index[x] *kernel_length : (filter_index[x]+1) *kernel_length] = 0\n",
    "\n",
    "            print(\"filter codebook done\")\n",
    "        else:\n",
    "            pass\n",
    "        return codebook\n",
    "\n",
    "    # 将x转换为tensor的float类型\n",
    "    def convert2tensor(self,x):\n",
    "        x = torch.FloatTensor(x)\n",
    "        return x\n",
    "    \n",
    "    def init_length(self):\n",
    "        for index, item in enumerate(self.model.parameters()):\n",
    "            self.model_size [index] = item.size()\n",
    "        \n",
    "        for index1 in self.model_size:\n",
    "            for index2 in range(0,len(self.model_size[index1])):\n",
    "                if index2 ==0:\n",
    "                    self.model_length[index1] = self.model_size[index1][0]\n",
    "                else:\n",
    "                    self.model_length[index1] *= self.model_size[index1][index2]\n",
    "                    \n",
    "    def init_rate(self, layer_rate):\n",
    "        for index, item in enumerate(self.model.parameters()):\n",
    "            self.compress_rate[index] = 1\n",
    "        for key in range(args.layer_begin, args.layer_end + 1, args.layer_inter):\n",
    "            self.compress_rate[key] = layer_rate\n",
    "        # different setting for different architecture\n",
    "        if args.arch == 'resnet20':\n",
    "            last_index = 57\n",
    "        elif args.arch == 'resnet32':\n",
    "            last_index = 93\n",
    "        elif args.arch == 'resnet56':\n",
    "            last_index = 165\n",
    "        elif args.arch == 'resnet110':\n",
    "            last_index = 327\n",
    "        self.mask_index =[x for x in range (0,last_index,3)]\n",
    "#        self.mask_index =  [x for x in range (0,330,3)]\n",
    "    # 代码中的mask为是否对卷积层进行掩盖，即剪枝\n",
    "    def init_mask(self,layer_rate):\n",
    "        self.init_rate(layer_rate)\n",
    "        for index, item in enumerate(self.model.parameters()):\n",
    "            if(index in self.mask_index):\n",
    "                self.mat[index] = self.get_filter_codebook(item.data, self.compress_rate[index],self.model_length[index] )\n",
    "                self.mat[index] = self.convert2tensor(self.mat[index])\n",
    "                if args.use_cuda:\n",
    "                    self.mat[index] = self.mat[index].cuda()\n",
    "        print(\"mask Ready\")\n",
    "\n",
    "    def do_mask(self):\n",
    "        for index, item in enumerate(self.model.parameters()):\n",
    "            if(index in self.mask_index):\n",
    "                a = item.data.view(self.model_length[index])\n",
    "                b = a * self.mat[index]\n",
    "                item.data = b.view(self.model_size[index])\n",
    "        print(\"mask Done\")\n",
    "\n",
    "    def if_zero(self):\n",
    "        for index, item in enumerate(self.model.parameters()):\n",
    "#            if(index in self.mask_index):\n",
    "            if(index ==0):\n",
    "                a = item.data.view(self.model_length[index])\n",
    "                b = a.cpu().numpy()\n",
    "                \n",
    "                print(\"number of nonzero weight is %d, zero is %d\" %( np.count_nonzero(b),len(b)- np.count_nonzero(b)))\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679aa7be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
